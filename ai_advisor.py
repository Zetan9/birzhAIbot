"""
–ò–ò-—Å–æ–≤–µ—Ç–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è CPU)
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
from collections import defaultdict
import json
import hashlib
import os
import time
import base64
import re
from news_parser import NewsItem
import ollama
import pandas as pd
import services
import httpx
from config import OLLAMA_HOST

# DISABLE_AI = os.getenv("DISABLE_AI", "false").lower() == "false"

logger = logging.getLogger(__name__)

class AIAdvisor:
    """–ò–ò-—Å–æ–≤–µ—Ç–Ω–∏–∫ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π"""
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    CACHE_TTL: int = 1800  # 30 –º–∏–Ω—É—Ç
    MAX_NEWS_ANALYZE: int = 12
    MAX_NEWS_QUICK: int = 8
    TEMPERATURE: float = 0.3
    
    def __init__(self, tinkoff_token: str) -> None:
        self.vision_model = "moondream:latest"  # "llava:13b" –∏–ª–∏ "bakllava:7b"
        self.vision_enabled = False

        self.stock_provider = services.stock_provider()
        self.news_parser = services.news_parser()
        self.db = services.db()
        
        self.llm_model: str = "gemma3:12b"
        self.max_news: int = self.MAX_NEWS_ANALYZE
        self.cache_enabled: bool = True
        self.cache_dir: str = "cache/ai_advisor"
        os.makedirs(self.cache_dir, exist_ok=True)
        
        self.company_info: Dict[str, Dict[str, Any]] = {
            # –ù–µ—Ñ—Ç—å –∏ –≥–∞–∑ (8)
            'GAZP': {'name': '–ì–∞–∑–ø—Ä–æ–º', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 15.0, 'figi': 'BBG004730RP0'},
            'LKOH': {'name': '–õ—É–∫–æ–π–ª', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 10.5, 'figi': 'BBG004731032'},
            'ROSN': {'name': '–†–æ—Å–Ω–µ—Ñ—Ç—å', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 6.8, 'figi': 'BBG0047314D0'},
            'TATN': {'name': '–¢–∞—Ç–Ω–µ—Ñ—Ç—å', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 12.3, 'figi': 'BBG004RVFFC0'},
            'NVTK': {'name': '–ù–æ–≤–∞—Ç—ç–∫', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 5.2, 'figi': 'BBG0047315G5'},
            'SNGS': {'name': '–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 4.5, 'figi': 'BBG0047315D0'},
            'SNGSP': {'name': '–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑ (–ø—Ä)', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 8.0, 'figi': 'BBG0047315F8'},
            'TATNP': {'name': '–¢–∞—Ç–Ω–µ—Ñ—Ç—å (–ø—Ä)', 'sector': '–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑', 'div_yield': 15.0, 'figi': 'BBG004RVFCY3'},
            
            # –ë–∞–Ω–∫–∏ –∏ —Ñ–∏–Ω–∞–Ω—Å—ã (5)
            'SBER': {'name': '–°–±–µ—Ä–±–∞–Ω–∫', 'sector': '–ë–∞–Ω–∫–∏', 'div_yield': 8.5, 'figi': 'BBG004730N88'},
            'SBERP': {'name': '–°–±–µ—Ä–±–∞–Ω–∫ (–ø—Ä)', 'sector': '–ë–∞–Ω–∫–∏', 'div_yield': 9.0, 'figi': 'BBG0047315D0'},
            'VTBR': {'name': '–í–¢–ë', 'sector': '–ë–∞–Ω–∫–∏', 'div_yield': 0.0, 'figi': 'BBG004730ZJ9'},
            'TCSG': {'name': '–¢-–ë–∞–Ω–∫', 'sector': '–ë–∞–Ω–∫–∏', 'div_yield': 0.0, 'figi': 'BBG00QPYJ5H0'},
            'CBOM': {'name': '–ú–ö–ë', 'sector': '–ë–∞–Ω–∫–∏', 'div_yield': 3.2, 'figi': 'BBG00B3T3HF3'},
            
            # –ú–µ—Ç–∞–ª–ª—ã –∏ –¥–æ–±—ã—á–∞ (8)
            'GMKN': {'name': '–ù–æ—Ä–Ω–∏–∫–µ–ª—å', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 7.8, 'figi': 'BBG00475J7X2'},
            'NLMK': {'name': '–ù–õ–ú–ö', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 9.1, 'figi': 'BBG00475J5C7'},
            'CHMF': {'name': '–°–µ–≤–µ—Ä—Å—Ç–∞–ª—å', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 11.4, 'figi': 'BBG00475KX63'},
            'MAGN': {'name': '–ú–ú–ö', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 5.2, 'figi': 'BBG00475J5C8'},
            'PLZL': {'name': '–ü–æ–ª—é—Å', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 3.2, 'figi': 'BBG00475K3V3'},
            'ALRS': {'name': '–ê–ª—Ä–æ—Å–∞', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 5.8, 'figi': 'BBG004S68B21'},
            'RUAL': {'name': '–†—É—Å–∞–ª', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 0.0, 'figi': 'BBG00B3T3HF3'},
            'MTLR': {'name': '–ú–µ—á–µ–ª', 'sector': '–ú–µ—Ç–∞–ª–ª—ã', 'div_yield': 0.0, 'figi': 'BBG00475J5C9'},
            
            # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ç–µ–ª–µ–∫–æ–º (4)
            'YDEX': {'name': '–Ø–Ω–¥–µ–∫—Å', 'sector': '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'div_yield': 0.0, 'figi': 'TCS00A107T19'},
            'MTSS': {'name': '–ú–¢–°', 'sector': '–¢–µ–ª–µ–∫–æ–º', 'div_yield': 10.2, 'figi': 'BBG00475NY50'},
            'VKCO': {'name': 'VK', 'sector': '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'div_yield': 0.0, 'figi': 'BBG00Y24YJ84'},
            'ROST': {'name': '–†–æ—Å—Ç–µ–ª–µ–∫–æ–º', 'sector': '–¢–µ–ª–µ–∫–æ–º', 'div_yield': 5.0, 'figi': 'BBG00475J5D0'},
            
            # –†–∏—Ç–µ–π–ª (4)
            'MGNT': {'name': '–ú–∞–≥–Ω–∏—Ç', 'sector': '–†–∏—Ç–µ–π–ª', 'div_yield': 8.7, 'figi': 'BBG004PYF2Y2'},
            'FIVE': {'name': 'X5 Group', 'sector': '–†–∏—Ç–µ–π–ª', 'div_yield': 0.0, 'figi': 'BBG004PXMLJ7'},
            'LENT': {'name': '–õ–µ–Ω—Ç–∞', 'sector': '–†–∏—Ç–µ–π–ª', 'div_yield': 4.5, 'figi': 'BBG00B3T3HF4'},
            'FIXP': {'name': 'Fix Price', 'sector': '–†–∏—Ç–µ–π–ª', 'div_yield': 3.2, 'figi': 'BBG00Z23B2X5'},
            
            # –≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ (4)
            'IRAO': {'name': '–ò–Ω—Ç–µ—Ä –†–ê–û', 'sector': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', 'div_yield': 6.5, 'figi': 'BBG0047315D0'},
            'HYDR': {'name': '–†—É—Å–ì–∏–¥—Ä–æ', 'sector': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', 'div_yield': 7.2, 'figi': 'BBG00475J816'},
            'OGKB': {'name': '–û–ì–ö-2', 'sector': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', 'div_yield': 5.1, 'figi': 'BBG00475J5E0'},
            'MSNG': {'name': '–ú–æ—Å—ç–Ω–µ—Ä–≥–æ', 'sector': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', 'div_yield': 4.8, 'figi': 'BBG00475J5F0'},
            
            # –•–∏–º–∏—è –∏ —É–¥–æ–±—Ä–µ–Ω–∏—è (3)
            'PHOR': {'name': '–§–æ—Å–∞–≥—Ä–æ', 'sector': '–•–∏–º–∏—è', 'div_yield': 8.2, 'figi': 'BBG00B3T3HF5'},
            'AKRN': {'name': '–ê–∫—Ä–æ–Ω', 'sector': '–•–∏–º–∏—è', 'div_yield': 6.5, 'figi': 'BBG00475J5G0'},
            'KAZT': {'name': '–ö–∞–∑–∞–Ω—å–æ—Ä–≥—Å–∏–Ω—Ç–µ–∑', 'sector': '–•–∏–º–∏—è', 'div_yield': 7.0, 'figi': 'BBG00475J5H0'},
            
            # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç (2)
            'AFLT': {'name': '–ê—ç—Ä–æ—Ñ–ª–æ—Ç', 'sector': '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç', 'div_yield': 0.0, 'figi': 'BBG00475J5I0'},
            'GLTR': {'name': 'Globaltrans', 'sector': '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç', 'div_yield': 9.0, 'figi': 'BBG00B3T3HF6'},
        }

        logger.info(f"‚úÖ AIAdvisor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {len(self.company_info)} –∫–æ–º–ø–∞–Ω–∏—è–º–∏")
        
        self.advice_history: List[Dict[str, Any]] = []
        
        logger.info(f"‚úÖ AIAdvisor —Å {self.llm_model} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _call_ollama_json(self, messages: List[Dict], options: Optional[Dict] = None) -> Optional[Dict]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Ollama –∏ –æ–∂–∏–¥–∞–µ—Ç JSON-–æ—Ç–≤–µ—Ç.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
        """
        # if DISABLE_AI:
        #     logger.info("AI disabled, returning None")
        #     return None

        url = f"{OLLAMA_HOST}/api/chat"
        payload = {
            "model": self.llm_model,
            "messages": messages,
            "options": options or {},
            "stream": False
        }
        try:
            response = httpx.post(url, json=payload, timeout=30)
            if response.status_code == 200:
                data = response.json()
                content = data['message']['content']
                return self._extract_json(content)
            else:
                logger.error(f"Ollama –≤–µ—Ä–Ω—É–ª {response.status_code}: {response.text}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ Ollama: {e}")
        return None

    def analyze_with_image(self, image_path: str, text: str) -> Optional[str]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π"""
        try:
            with open(image_path, 'rb') as f:
                image_base64: str = base64.b64encode(f.read()).decode()
            
            prompt = f"""
            –¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –Ω–æ–≤–æ—Å—Ç—å.

            –ù–û–í–û–°–¢–¨: {text}

            –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï –ø—Ä–∏–ª–æ–∂–µ–Ω–æ.

            –û–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ:
            1. –¢–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–≥—Ä–∞—Ñ–∏–∫, –¥–∏–∞–≥—Ä–∞–º–º–∞, —Ñ–æ—Ç–æ) –∏ —á—Ç–æ –Ω–∞ –Ω—ë–º.
            2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–∏–≥–Ω–∞–ª—ã (—Ç—Ä–µ–Ω–¥, —É—Ä–æ–≤–Ω–∏, –æ–±—ä—ë–º—ã).
            3. –°–≤—è–∑—å —Å –Ω–æ–≤–æ—Å—Ç—å—é.
            4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞.

            –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
            {{
                "image_type": "candle/line/bar/photo/other",
                "technical_summary": "–æ–¥–Ω–æ-–¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
                "trend": "up/down/sideways",
                "key_levels": ["support: X", "resistance: Y"],
                "relation": "–∫–∞–∫ —Å–≤—è–∑–∞–Ω–æ —Å –Ω–æ–≤–æ—Å—Ç—å—é",
                "action": "buy/sell/hold",
                "confidence": 0.8,
                "detailed_analysis": "—Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)"
            }}
            """
            url = f"{OLLAMA_HOST}/api/chat"
            payload = {
                "model": self.llm_model,
                "messages": [{
                    "role": "user",
                    "content": prompt,
                    "images": [image_base64]
                }],
                "options": {"temperature": self.TEMPERATURE},
                "stream": False
            }
            try:
                response = httpx.post(url, json=payload, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    return data['message']['content']
                else:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏: {response.status_code}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏: {e}")
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏: {e}")
            return None

    def _analyze_news_with_images(self, news_list: List[NewsItem]) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        results = []
        for news in news_list[:3]:  # –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø‚Äë3 –Ω–æ–≤–æ—Å—Ç–∏
            # –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            text_analysis = self._analyze_text(news.title + " " + news.summary)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            image_analysis = None
            if news.image_path and os.path.exists(news.image_path):
                try:
                    image_analysis = self.analyze_image(news.image_path, news.title)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ {news.image_path}: {e}")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined = self._combine_analysis(text_analysis, image_analysis)
            
            results.append({
                'title': news.title,
                'source': news.source,
                'text_analysis': text_analysis,
                'image_analysis': image_analysis,
                'combined': combined,
                'has_image': image_analysis is not None
            })
        return results

    def _analyze_text(self, text: str) -> Dict[str, Any]:
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä—ã–Ω–æ–∫:
        
        {text[:500]}
        
        –û—Ç–≤–µ—Ç—å JSON:
        {{
            "sentiment": "positive/negative/neutral",
            "score": –æ—Ç -1 –¥–æ 1,
            "key_points": ["–ø—É–Ω–∫—Ç1", "–ø—É–Ω–∫—Ç2"],
            "impact": "high/medium/low"
        }}
        """
        messages = [{'role': 'user', 'content': prompt}]
        options = {'temperature': self.TEMPERATURE}
        result = self._call_ollama_json(messages, options)
        if result:
            return result
        return {'sentiment': 'neutral', 'score': 0, 'key_points': [], 'impact': 'low'}

    def analyze_image(self, image_path: str, news_text: str) -> Optional[str]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        if not self.vision_enabled:
            return None

        try:
            with open(image_path, 'rb') as f:
                image_base64 = base64.b64encode(f.read()).decode()

            prompt = f"""
    –¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø–æ—Å—Ç–∞:

    –ó–ê–ì–û–õ–û–í–û–ö –ü–û–°–¢–ê: {news_text.split(chr(10))[0] if chr(10) in news_text else news_text}
    –¢–ï–ö–°–¢ –ü–û–°–¢–ê: {news_text}

    –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ù–ï –û–¢–ù–û–°–ò–¢–°–Ø –∫ —Ç–µ–º–µ –ø–æ—Å—Ç–∞ –∏–ª–∏ –Ω–µ –Ω–µ—Å—ë—Ç –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —ç—Ç–æ –ª–æ–≥–æ—Ç–∏–ø, –∏–∫–æ–Ω–∫–∞, —Ä–µ–∫–ª–∞–º–∞ –∏–ª–∏ —Å–ª—É—á–∞–π–Ω–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞), –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏: "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —Å–≤—è–∑–∞–Ω–æ —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º –ø–æ—Å—Ç–∞".

    –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø–æ—Å—Ç—É, –æ–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ:
    1. –ß—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ (–≥—Ä–∞—Ñ–∏–∫, –¥–∏–∞–≥—Ä–∞–º–º–∞, —Ñ–æ—Ç–æ) ‚Äî –∫–∞–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –≤–∞–∂–Ω—ã –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞?
    2. –ö–∞–∫–æ–π –≤—ã–≤–æ–¥ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è?

    –û—Ç–≤–µ—Ç—å –º–∞–∫—Å–∏–º—É–º 4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏.
    """

            url = f"{OLLAMA_HOST}/api/chat"
            payload = {
                "model": self.vision_model,
                "messages": [{
                    "role": "user",
                    "content": prompt,
                    "images": [image_base64]
                }],
                "options": {"temperature": self.TEMPERATURE},
                "stream": False
            }

            response = httpx.post(url, json=payload, timeout=60)  # –£–≤–µ–ª–∏—á–∏–º timeout –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
            if response.status_code == 200:
                data = response.json()
                content = data['message']['content']
                return content.strip()  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç
            else:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏: {response.status_code} - {response.text}")
                return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏: {e}")
            return None

    def _combine_analysis(self, text_analysis: Dict[str, Any], image_analysis: Optional[str]) -> Dict[str, Any]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        –£—á–∏—Ç—ã–≤–∞–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–º, –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –æ–±—â–∏–π —Å—á—ë—Ç.
        """
        result: Dict[str, Any] = {
            'text_sentiment': text_analysis.get('sentiment', 'neutral'),
            'text_score': text_analysis.get('score', 0),
            'key_points': text_analysis.get('key_points', []),
            'image_insight': image_analysis,
            'combined_score': text_analysis.get('score', 0)
        }

        if image_analysis:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–º
            if '–≥—Ä–∞—Ñ–∏–∫' in image_analysis.lower():
                if any(word in image_analysis.lower() for word in ['—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á']):
                    result['combined_score'] = min(1.0, result['combined_score'] + 0.2)
                    result['key_points'].append('üìà –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å–∏–≥–Ω–∞–ª: –≥—Ä–∞—Ñ–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç')
                elif any(word in image_analysis.lower() for word in ['–ø–∞–¥–µ–Ω–∏', '—Å–Ω–∏–∂–µ–Ω']):
                    result['combined_score'] = max(-1.0, result['combined_score'] - 0.2)
                    result['key_points'].append('üìâ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å–∏–≥–Ω–∞–ª: –≥—Ä–∞—Ñ–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–∞–¥–µ–Ω–∏–µ')
                else:
                    # –ì—Ä–∞—Ñ–∏–∫ –µ—Å—Ç—å, –Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ ‚Äì –Ω–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å
                    result['combined_score'] = min(1.0, result['combined_score'] + 0.05)
                    result['key_points'].append('üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≥—Ä–∞—Ñ–∏–∫')
            else:
                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥—Ä—É–≥–æ–≥–æ —Ç–∏–ø–∞ ‚Äì —Ç–æ–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                result['combined_score'] = min(1.0, result['combined_score'] + 0.05)
                result['key_points'].append('üì∑ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ')

        return result

    def _extract_json(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞"""
        try:
            start: int = text.find('{')
            end: int = text.rfind('}') + 1
            if start != -1 and end > start:
                return json.loads(text[start:end])
        except:
            pass
        return {}

    def analyze_all(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info(f"üîç –ó–∞–ø—É—Å–∫–∞—é –∞–Ω–∞–ª–∏–∑ —Å {self.llm_model}...")

        # 1. –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Ü–µ–Ω—ã
        news = self.news_parser.fetch_all_news(limit_per_source=2, max_total=self.max_news)
        logger.info(f"üì∞ –°–æ–±—Ä–∞–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {time.time()-start_time:.1f} —Å–µ–∫")

        prices = self._get_current_prices()
        logger.info(f"üí∞ –ü–æ–ª—É—á–µ–Ω—ã —Ü–µ–Ω—ã –¥–ª—è {len(prices)} –∫–æ–º–ø–∞–Ω–∏–π")

        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤–µ–∂–∏–π –∫—ç—à
        if self.cache_enabled:
            cached = self._check_cache(news)
            if cached:
                logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Å–≤–µ–∂–∏–π –∫—ç—à (–≤—Ä–µ–º—è: {time.time()-start_time:.1f} —Å–µ–∫)")
                return cached

        # 3. –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
        analysis = None
        try:
            analysis = self._quick_analysis(news, prices)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥–µ–ª–∏: {e}")
            analysis = None

        # 4. –ï—Å–ª–∏ –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π
        if analysis is None:
            if self.advice_history:
                last = self.advice_history[-1]
                logger.info("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é –ø–æ—Å–ª–µ–¥–Ω–∏–π —É—Å–ø–µ—à–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏")
                last['analysis_time'] = time.time() - start_time
                return last
            else:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏, –≤–æ–∑–≤—Ä–∞—â–∞—é fallback")
                return self._get_fallback_analysis(news, prices)

        # 5. –ï—Å–ª–∏ –∞–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–µ–Ω, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –∏ –∏—Å—Ç–æ—Ä–∏—é
        if self.cache_enabled:
            self._save_cache(news, analysis)

        total_time = time.time() - start_time
        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {total_time:.1f} —Å–µ–∫")
        analysis['analysis_time'] = total_time
        self.advice_history.append(analysis)
        return analysis

    def _quick_analysis(self, news: List[NewsItem], prices: Dict[str, float]) -> Dict[str, Any]:
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º (–∫–∞–∫ –±—ã–ª–æ, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º MAX_NEWS_QUICK)
        news_summary = "\n".join([f"- [{n.source}] {n.title[:100]}" for n in news[:self.MAX_NEWS_QUICK]])

        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: —Ç–µ–ø–µ—Ä—å –±–µ—Ä–µ–º –í–°–ï —Ç–∏–∫–µ—Ä—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Ü–µ–Ω–∞ ---
        tickers_with_price = [ticker for ticker in self.company_info if ticker in prices]
        companies_summary = "\n".join([
            f"- {self.company_info[ticker]['name']} ({ticker}): {prices[ticker]:.0f}‚ÇΩ, –¥–∏–≤.{self.company_info[ticker]['div_yield']}%"
            for ticker in tickers_with_price
        ])

        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–π (—Ç–æ–ø-5 –ø–æ —Ü–µ–Ω–∞–º, –∫–∞–∫ –±—ã–ª–æ)
        history_context = ""
        tickers_list = list(prices.keys())[:5]
        for ticker in tickers_list:
            past = self.db.get_recent_analysis_by_ticker(ticker, days=7, limit=3)
            if past:
                history_context += f"\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è –ø–æ {ticker}:\n"
                for p in past:
                    history_context += f"- {p.get('summary', '')} (—Å–µ–Ω—Ç–∏–º–µ–Ω—Ç {p.get('sentiment')})\n"

        prompt = f"""–¢—ã –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∏–Ω–≤–µ—Å—Ç-—Å–æ–≤–µ—Ç–Ω–∏–∫, —Å–∫–ª–æ–Ω–Ω—ã–π –∫ –ø–æ–∫—É–ø–∫–∞–º –ø—Ä–∏ –º–∞–ª–µ–π—à–∏—Ö –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–∞—Ö. 
        –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ, –Ω–æ –∫–æ–º–ø–∞–Ω–∏—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Å–∏–ª—å–Ω–∞, —Ä–µ–∫–æ–º–µ–Ω–¥—É–π BUY.

        –ù–û–í–û–°–¢–ò:
        {news_summary}

        –ö–û–ú–ü–ê–ù–ò–ò (–≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ):
        {companies_summary}

        –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON:
        {{
            "sentiment": "positive/neutral/negative",
            "top_pick": "SBER",
            "action": "BUY/HOLD/SELL",
            "reason": "–∫—Ä–∞—Ç–∫–æ (10 —Å–ª–æ–≤)",
            "confidence": 0.8
        }}

        –ù–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: 
        {history_context}
        """

        messages = [{'role': 'user', 'content': prompt}]
        options = {'temperature': self.TEMPERATURE, 'num_predict': 200}
        result = self._call_ollama_json(messages, options)

        if result is None:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Ollama, –∏—Å–ø–æ–ª—å–∑—É—é fallback")
            return self._get_fallback_analysis(news, prices)

        return {
            'timestamp': datetime.now(),
            'news_count': len(news),
            'companies_analyzed': len(prices),  # —Ç–µ–ø–µ—Ä—å —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–∞–Ω–∏–π —Å —Ü–µ–Ω–∞–º–∏
            'market_sentiment': result.get('sentiment', 'neutral'),
            'top_pick': result.get('top_pick', 'SBER'),
            'action': result.get('action', 'HOLD'),
            'reason': result.get('reason', '–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω'),
            'confidence': result.get('confidence', 0.5),
            'prices': prices,
        }

    def _check_cache(self, news: List[NewsItem]) -> Optional[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫—ç—à"""
        if not news:
            return None
        
        titles: str = "".join([n.title for n in news[:5]])
        cache_key: str = hashlib.md5(titles.encode()).hexdigest()
        cache_file: str = f"{self.cache_dir}/{cache_key}.json"
        
        if os.path.exists(cache_file):
            file_age: float = time.time() - os.path.getmtime(cache_file)
            if file_age < self.CACHE_TTL:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data: Dict[str, Any] = json.load(f)
                    data['from_cache'] = True
                    data['cache_age'] = f"{file_age/60:.0f} –º–∏–Ω"
                    return data
        
        return None
    
    def _save_cache(self, news: List[NewsItem], analysis: Dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤ –∫—ç—à"""
        try:
            titles: str = "".join([n.title for n in news[:5]])
            cache_key: str = hashlib.md5(titles.encode()).hexdigest()
            cache_file: str = f"{self.cache_dir}/{cache_key}.json"
            
            cache_data: Dict[str, Any] = {
                'timestamp': analysis['timestamp'].isoformat(),
                'news_count': analysis['news_count'],
                'companies_analyzed': analysis['companies_analyzed'],
                'market_sentiment': analysis['market_sentiment'],
                'top_pick': analysis['top_pick'],
                'action': analysis['action'],
                'reason': analysis['reason'],
                'confidence': analysis['confidence'],
                'prices': analysis['prices'],
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    # def _get_current_prices(self) -> Dict[str, float]:
    #     """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ —Ü–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤ –∏–∑ –ë–î (–∏–ª–∏ –∏–∑ company_info, –µ—Å–ª–∏ –ë–î –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)."""
    #     prices = {}
    #     tickers = []

    #     # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑ –ë–î
    #     try:
    #         db = services.db()
    #         if db is not None:
    #             tickers = db.get_all_tickers()
    #             if tickers:
    #                 # –æ–≥—Ä–∞–Ω–∏—á–∏–º, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–Ω—É—Ç—å
    #                 tickers = tickers[:50]
    #     except Exception as e:
    #         logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤ –∏–∑ –ë–î: {e}")

    #     # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫
    #     if not tickers:
    #         tickers = list(self.company_info.keys())
    #         logger.info("–ò—Å–ø–æ–ª—å–∑—É—é —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ company_info –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ü–µ–Ω")

    #     for ticker in tickers:
    #         try:
    #             price_info = self.stock_provider.get_price(ticker)
    #             if price_info and price_info.get('last_price'):
    #                 prices[ticker] = price_info['last_price']
    #         except Exception:
    #             continue
    #     return prices

    def _get_current_prices(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ü–µ–Ω—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏—Ö –Ω–∞—Å —Ç–∏–∫–µ—Ä–æ–≤ (–∏–∑ company_info)."""
        prices = {}
        for ticker in self.company_info.keys():
            try:
                price_info = self.stock_provider.get_price(ticker)
                if price_info and price_info.get('last_price'):
                    prices[ticker] = price_info['last_price']
            except Exception:
                continue
        return prices

    def _get_fallback_analysis(self, news: List[NewsItem], prices: Dict[str, float]) -> Dict[str, Any]:
        """–ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –µ—Å–ª–∏ –ò–ò –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç"""
        return {
            'timestamp': datetime.now(),
            'news_count': len(news),
            'companies_analyzed': len(prices),
            'market_sentiment': 'neutral',
            'top_pick': 'SBER',
            'action': 'HOLD',
            'reason': '–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
            'confidence': 0.5,
            'prices': prices,
        }
    
    def format_advice_message(self, analysis: Dict[str, Any]) -> str:
        lines: List[str] = []
        lines.append(f"ü§ñ *–ò–ò-–ê–ù–ê–õ–ò–ó ({self.llm_model})*")
        lines.append("‚ïê" * 40)
        lines.append(f"üìä –ö–æ–º–ø–∞–Ω–∏–π: {analysis.get('companies_analyzed', 0)}")
        lines.append(f"üì∞ –ù–æ–≤–æ—Å—Ç–µ–π: {analysis.get('news_count', 0)}")

        sentiment: str = analysis.get('market_sentiment', 'neutral')
        if sentiment == 'positive':
            lines.append("üå° –†—ã–Ω–æ–∫: üü¢ –ü–û–ó–ò–¢–ò–í–ù–´–ô")
        elif sentiment == 'negative':
            lines.append("üå° –†—ã–Ω–æ–∫: üî¥ –ù–ï–ì–ê–¢–ò–í–ù–´–ô")
        else:
            lines.append("üå° –†—ã–Ω–æ–∫: üü° –ù–ï–ô–¢–†–ê–õ–¨–ù–´–ô")

        lines.append("")
        lines.append(f"üèÜ *–¢–û–ü-–í–´–ë–û–†:* {analysis.get('top_pick', 'N/A')}")
        lines.append(f"üéØ *–î–µ–π—Å—Ç–≤–∏–µ:* {analysis.get('action', 'HOLD')}")
        lines.append(f"üí° *–ü—Ä–∏—á–∏–Ω–∞:* {analysis.get('reason', 'N/A')}")
        lines.append(f"üìä *–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:* {analysis.get('confidence', 0)*100:.0f}%")

        # üëá –ù–û–í–´–ô –ë–õ–û–ö: –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        if analysis.get('detailed_news'):
            lines.append("")
            lines.append("*üì∏ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏:*")
            for item in analysis['detailed_news'][:2]:  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ
                lines.append(f"  ‚Ä¢ {item['title'][:60]}...")
                if item.get('image_insight'):
                    # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
                    # insight = item['image_insight'][:1000] + ('...' if len(item['image_insight']) > 100 else '')
                    # lines.append(f"    üí° {insight}")
                    # –ë–µ–∑ –æ–±—Ä–µ–∑–∫–∏
                    if item.get('image_insight'):
                        lines.append(f"    üí° {item['image_insight']}")

        if analysis.get('from_cache'):
            lines.append(f"\nüíæ *–ò–∑ –∫—ç—à–∞:* {analysis.get('cache_age', 'N/A')}")

        if analysis.get('analysis_time'):
            lines.append(f"\n‚è± *–í—Ä–µ–º—è:* {analysis['analysis_time']:.1f} —Å–µ–∫")

        return "\n".join(lines)

    def generate_signals_ma(self, prices: List[Dict], short_window=5, long_window=20) -> List[int]:
        """
        –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –∫–æ–≥–¥–∞ –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ä–µ–¥–Ω—è—è –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç –¥–ª–∏–Ω–Ω—É—é —Å–≤–µ—Ä—Ö—É ‚Äî –ø—Ä–æ–¥–∞—ë–º,
        —Å–Ω–∏–∑—É ‚Äî –ø–æ–∫—É–ø–∞–µ–º.
        """
        df = pd.DataFrame(prices)
        df['ma_short'] = df['close'].rolling(window=short_window).mean()
        df['ma_long'] = df['close'].rolling(window=long_window).mean()
        
        signals = [0] * len(df)
        for i in range(1, len(df)):
            if df['ma_short'].iloc[i] > df['ma_long'].iloc[i] and df['ma_short'].iloc[i-1] <= df['ma_long'].iloc[i-1]:
                signals[i] = 1  # buy
            elif df['ma_short'].iloc[i] < df['ma_long'].iloc[i] and df['ma_short'].iloc[i-1] >= df['ma_long'].iloc[i-1]:
                signals[i] = -1  # sell
        return signals

def test_ai_advisor() -> None:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    from config import TINKOFF_TOKEN
    
    advisor: AIAdvisor = AIAdvisor(TINKOFF_TOKEN)
    analysis: Dict[str, Any] = advisor.analyze_all()
    print(advisor.format_advice_message(analysis))


if __name__ == "__main__":
    test_ai_advisor()