"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å–±–æ—Ä–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
"""

import feedparser
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Any
import logging
from dataclasses import dataclass, field
import requests
import hashlib
import re
import dateutil.parser
import os
import uuid
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import requests

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏"""
    source: str
    title: str
    summary: str
    link: str
    published: datetime
    related_tickers: List[str] = field(default_factory=list)
    sentiment_score: Optional[float] = None
    importance: float = 1.0
    language: str = 'ru'
    category: str = 'unknown'  # finance, economy, politics, other
    image_path: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'source': self.source,
            'title': self.title,
            'summary': self.summary,
            'link': self.link,
            'published': self.published,
            'related_tickers': self.related_tickers,
            'sentiment_score': self.sentiment_score,
            'importance': self.importance,
            'language': self.language,
            'category': self.category
        }

class NewsParser:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self, finance_only: bool = True):
        self.finance_only = finance_only
        
        # –¢–û–õ–¨–ö–û –†–û–°–°–ò–ô–°–ö–ò–ï –ò–°–¢–û–ß–ù–ò–ö–ò (–º–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å)
        self.rss_sources = {
            # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            'cbr': 'https://www.cbr.ru/rss/eventrss',
            'minfin': 'https://minfin.gov.ru/ru/rss/',
            'economy': 'https://economy.gov.ru/rss/feed',
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞
            'interfax': 'http://www.interfax.ru/rss.asp',
            'tass': 'http://tass.ru/rss/v2.xml',
            'ria': 'https://ria.ru/export/rss2/economy/index.xml',
            'prime': 'https://1prime.ru/export/rss2/index.xml',
            'finmarket': 'https://www.finmarket.ru/rss/mainnews.asp',
            'finam': 'https://www.finam.ru/analysis/conferences/export/rsspoint.asp',
            
            # –î–µ–ª–æ–≤—ã–µ –°–ú–ò
            'rbc': 'https://rssexport.rbc.ru/rbcnews/news/30/full.sn',
            'kommersant': 'https://www.kommersant.ru/RSS/main.xml',
            'vedomosti': 'https://vedomosti.ru/rss/news',
            'forbes_russia': 'https://www.forbes.ru/rss',
            'bfm': 'https://www.bfm.ru/rss',
            'dp': 'https://www.dp.ru/rss/all.xml',
            
            # –ù–µ—Ñ—Ç—å –∏ –≥–∞–∑
            'oilru': 'https://oilru.com/news/rss/',
            'neftegaz': 'https://neftegaz.ru/export/rss/',
            'oilcapital': 'https://oilcapital.ru/export/rss/',
            
            # –ú–µ—Ç–∞–ª–ª—ã –∏ –¥–æ–±—ã—á–∞
            'metaltorg': 'https://www.metaltorg.ru/export/rss/',
            
            # –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ä—Ç–∞–ª—ã
            'investing': 'https://www.investing.com/rss/news.rss',
            'smartlab': 'https://smart-lab.ru/rss/',
            'bcs_express': 'https://bcs-express.ru/rss/all',
            'tinkoff_invest': 'https://www.tinkoff.ru/api/v1/rss/invest',
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ —ç–∫–æ–Ω–æ–º–∏–∫–∞/—Ñ–∏–Ω–∞–Ω—Å—ã)
        self.finance_keywords = [
            # –†—É—Å—Å–∫–∏–µ
            '–∞–∫—Ü–∏', '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—Ä—ã–Ω–æ–∫', '–∏–Ω–≤–µ—Å—Ç–∏—Ü',
            '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–∫–∞–ø–∏—Ç–∞–ª', '–±–∏—Ä–∂–∞', '–∫–æ—Ç–∏—Ä–æ–≤–∫', '–∏–Ω–¥–µ–∫—Å',
            '—Å–±–µ—Ä', '–≥–∞–∑–ø—Ä–æ–º', '–ª—É–∫–æ–π–ª', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '—è–Ω–¥–µ–∫—Å', '–∞–∫—Ü–∏—è',
            '–¥–∏–≤–∏–¥–µ–Ω–¥', '–æ—Ç—á–µ—Ç', '—Ñ–∏–Ω–∞–Ω—Å', '—ç–∫–æ–Ω–æ–º–∏–∫', '–±–∏–∑–Ω–µ—Å', '—Ç–æ—Ä–≥–∏',
            '—Ä–æ—Å—Å', '–∫–æ–º–ø–∞–Ω–∏', '–∫–æ—Ä–ø–æ—Ä–∞—Ü', '–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '—Å—Ç–∞–≤–∫',
            '–º–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞', 'moex', 'rts', '–∏–Ω–≤–µ—Å—Ç–æ—Ä', '–ø–æ—Ä—Ç—Ñ–µ–ª—å',
            '—Ñ–æ–Ω–¥–æ–≤—ã–π', '–æ–±–ª–∏–≥–∞—Ü', '–≤–∞–ª—é—Ç–∞', '–∏–Ω—Ñ–ª—è—Ü', '–≤–≤–ø', '–±—é–¥–∂–µ—Ç',
            '–Ω–∞–ª–æ–≥', '–ø–æ—à–ª–∏–Ω', '—Å–∞–Ω–∫—Ü', '—ç–º–±–∞—Ä–≥–æ', '–¥–µ—Ñ–æ–ª—Ç', '–∫—Ä–∏–∑–∏—Å',
            '—Ä–µ—Ü–µ—Å—Å–∏', '—Å—Ç–∞–≤–∫–∞', '–∫–ª—é—á–µ–≤–∞—è', '—Ü–µ–±–æ', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫',
            '—Å–±–µ—Ä–±–∞–Ω–∫', '–≤—Ç–±', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '–º–æ—Å–±–∏—Ä–∂–∞', '—Å–ø–± –±–∏—Ä–∂–∞',
            
            # English
            'stock', 'market', 'invest', 'trading', 'finance', 'econom',
            'fed', 'federal reserve', 'inflation', 'gdp', 'oil', 'gas',
            'commodity', 'gold', 'silver', 'copper', 'bond', 'yield',
            'dividend', 'earnings', 'revenue', 'profit', 'loss',
            'bank', 'credit', 'loan', 'mortgage', 'rate', 'interest',
            'dollar', 'euro', 'currency', 'forex', 'crypto',
            'sberbank', 'gazprom', 'lukoil', 'yandex', 'rosneft',
            'moex', 'rts', 'micex', 'tinkoff', 'vtb'
        ]
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è)
        self.stop_keywords = [
            # –°–ø–æ—Ä—Ç
            '—Ñ—É—Ç–±–æ–ª', '—Ö–æ–∫–∫–µ–π', '—Ç–µ–Ω–Ω–∏—Å', '–æ–ª–∏–º–ø–∏–∞–¥', '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '—Ç—É—Ä–Ω–∏—Ä',
            '—Å–ø–æ—Ä—Ç', '–º–∞—Ç—á', '–∏–≥—Ä–æ–∫', '—Ç—Ä–µ–Ω–µ—Ä', '—Å—Ç–∞–¥–∏–æ–Ω', '–≥–æ–ª', '—Å—á–µ—Ç',
            
            # –®–æ—É-–±–∏–∑–Ω–µ—Å
            '–∞–∫—Ç—Ä–∏—Å', '–∞–∫—Ç–µ—Ä', '–ø–µ–≤–µ—Ü', '–ø–µ–≤–∏—Ü', '—Ñ–∏–ª—å–º', '–∫–∏–Ω–æ', '—Å–µ—Ä–∏–∞–ª',
            '—à–æ—É', '–≤–µ–¥—É—â', '–∑–≤–µ–∑–¥', '–∑–Ω–∞–º–µ–Ω–∏—Ç–æ—Å—Ç', '—Å–≤–µ—Ç—Å–∫–∞—è –∂–∏–∑–Ω—å',
            
            # –ü–æ–≥–æ–¥–∞
            '–ø–æ–≥–æ–¥', '–¥–æ–∂–¥—å', '—Å–Ω–µ–≥', '–≤–µ—Ç–µ—Ä', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–ø–æ—Ö–æ–ª–æ–¥–∞–Ω–∏–µ',
            '–ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ', '—Ü–∏–∫–ª–æ–Ω', '–∞–Ω—Ç–∏—Ü–∏–∫–ª–æ–Ω',
            
            # –†–∞–∑–Ω–æ–µ
            '—Ä–µ—Ü–µ–ø—Ç', '–∫—É–ª–∏–Ω–∞—Ä', '–∑–¥–æ—Ä–æ–≤—å–µ', '–º–µ–¥–∏—Ü–∏–Ω', '–∫–æ—Ä–æ–Ω–∞–≤–∏—Ä—É—Å',
            'covid', '–ø—Ä–∞–∑–¥–Ω–∏–∫', '–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ', '–¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è',
            '–≥–æ—Ä–æ—Å–∫–æ–ø', '–º–∞–≥–∏—è', '—ç–∑–æ—Ç–µ—Ä–∏–∫'
        ]
        
        # –¢–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ä—ã (—Ç–∏–∫–µ—Ä -> –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π)
        self.company_tickers = {
            'SBER': ['—Å–±–µ—Ä–±–∞–Ω–∫', '—Å–±–µ—Ä', 'sberbank', 'sber'],
            'VTBR': ['–≤—Ç–±', 'vtb', 'vtbr'],
            'TCSG': ['—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '—Ç–±–∞–Ω–∫', 'tinkoff', 'tcs'],
            'GAZP': ['–≥–∞–∑–ø—Ä–æ–º', 'gazprom'],
            'LKOH': ['–ª—É–∫–æ–π–ª', 'lukoil'],
            'ROSN': ['—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'rosneft'],
            'TATN': ['—Ç–∞—Ç–Ω–µ—Ñ—Ç—å', 'tatneft'],
            'NVTK': ['–Ω–æ–≤–∞—Ç—ç–∫', 'novatek'],
            'YDEX': ['—è–Ω–¥–µ–∫—Å', 'yandex', 'ydex'],
            'GMKN': ['–Ω–æ—Ä–Ω–∏–∫–µ–ª—å', 'nornickel'],
            'MTSS': ['–º—Ç—Å', 'mts'],
            'CHMF': ['—Å–µ–≤–µ—Ä—Å—Ç–∞–ª—å', 'severstal'],
            'NLMK': ['–Ω–ª–º–∫', 'nlmk'],
            'PLZL': ['–ø–æ–ª—é—Å', 'polyus'],
            'ALRS': ['–∞–ª—Ä–æ—Å–∞', 'alrosa'],
            'MGNT': ['–º–∞–≥–Ω–∏—Ç', 'magnit'],
            'FIVE': ['—Ö5', 'x5', '–ø—è—Ç–µ—Ä–æ—á–∫–∞'],
        }
        
        # –ö—ç—à –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        self.seen_links = set()
        self.seen_titles = set()
        
        logger.info(f"‚úÖ NewsParser –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {len(self.rss_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")
        if self.finance_only:
            logger.info("üí∞ –†–µ–∂–∏–º: —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏")

    def _safe_get_str(self, value, default: str = "") -> str:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è —Å–ø–∏—Å–∫–∏ –∏ None.
        """
        if value is None:
            return default
        if isinstance(value, list):
            # –ï—Å–ª–∏ –ø—Ä–∏—à—ë–ª —Å–ø–∏—Å–æ–∫, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–ª–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
            return str(value[0]) if value else default
        return str(value)

    def is_finance_news(self, title: str, summary: str = "") -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –∫ —Ñ–∏–Ω–∞–Ω—Å–∞–º/—ç–∫–æ–Ω–æ–º–∏–∫–µ"""
        text = (title + " " + summary).lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å - –Ω–µ —Ñ–∏–Ω–∞–Ω—Å—ã)
        for word in self.stop_keywords:
            if word in text:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤
        finance_count = 0
        for word in self.finance_keywords:
            if word in text:
                finance_count += 1
                if finance_count >= 2:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 2 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                    return True
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–∏–∫–µ—Ä—ã - —Ç–æ—á–Ω–æ —Ñ–∏–Ω–∞–Ω—Å—ã
        if self._find_tickers(text):
            return True
        
        return finance_count >= 1  # –•–æ—Ç—è –±—ã 1 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    
    def fetch_all_news(self, limit_per_source: int = 3, max_total: int = 50) -> List[NewsItem]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        all_news = []
        self.seen_links.clear()
        self.seen_titles.clear()
        
        logger.info(f"üì° –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {len(self.rss_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_source = {
                executor.submit(self.fetch_from_source, source_name, url, limit_per_source): source_name
                for source_name, url in self.rss_sources.items()
            }
            
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                try:
                    news_items = future.result(timeout=15)
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                    if self.finance_only:
                        finance_news = [
                            item for item in news_items 
                            if self.is_finance_news(item.title, item.summary)
                        ]
                        logger.info(f"‚úÖ {source_name}: {len(news_items)} ‚Üí {len(finance_news)} —Ñ–∏–Ω.")
                        all_news.extend(finance_news)
                    else:
                        all_news.extend(news_items)
                        
                except Exception as e:
                    logger.warning(f"‚ùå {source_name}: {e}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_news = self._deduplicate_news(all_news)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
        unique_news.sort(key=lambda x: x.published, reverse=True)
        
        logger.info(f"üì∞ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(unique_news)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return unique_news[:max_total]
    
    def _extract_image_from_url(self, url: str, source_name: str) -> Optional[str]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤–æ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None.
        """
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                return None
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò—â–µ–º Open Graph –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π)
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                img_url = str(og_image['content'])  # ‚Üê –¥–æ–±–∞–≤–∏—Ç—å str()
                return self._download_image(img_url, source_name)
            
            # –ò–Ω–∞—á–µ –∏—â–µ–º –ø–µ—Ä–≤—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ —Å—Ç–∞—Ç—å–µ (—Ç–µ–≥ <img>)
            img_tag = soup.find('img', class_=re.compile(r'(article|news|content|main)'))
            if not img_tag:
                img_tag = soup.find('img')
            if img_tag and img_tag.get('src'):
                img_url = str(img_tag['src'])  # ‚Üê –¥–æ–±–∞–≤–∏—Ç—å str()
                img_url = urljoin(url, img_url)
                return self._download_image(img_url, source_name)
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ {url}: {e}")
        return None

    def _download_image(self, img_url: str, source_name: str) -> Optional[str]:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ø–∞–ø–∫—É 'news_images'.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ None.
        """
        try:
            # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            os.makedirs('news_images', exist_ok=True)
            # –£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            ext = os.path.splitext(img_url.split('?')[0])[1]
            if not ext or ext.lower() not in ('.jpg', '.jpeg', '.png', '.gif'):
                ext = '.jpg'  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            filename = f"{source_name}_{uuid.uuid4().hex}{ext}"
            filepath = os.path.join('news_images', filename)
            
            response = requests.get(img_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filepath}")
                return filepath
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {img_url}: {e}")
        return None

    def fetch_from_source(self, source_name: str, url: str, limit: int) -> List[NewsItem]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        news_list = []
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            
            # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É
            if source_name == 'finmarket':
                # –î–ª—è finmarket –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–≤–∏–º windows-1251
                response.encoding = 'windows-1251'
            else:
                # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                response.encoding = response.apparent_encoding or 'utf-8'
            
            logger.info(f"üì° {source_name}: –∫–æ–¥–∏—Ä–æ–≤–∫–∞ {response.encoding}")
            feed = feedparser.parse(response.text)
            
            for entry in feed.entries[:limit]:
                try:
                    title = self._safe_get_str(entry.get('title'))
                    # summary –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –ø–æ–ª—è—Ö summary –∏–ª–∏ description
                    summary = self._safe_get_str(entry.get('summary'))
                    if not summary:
                        summary = self._safe_get_str(entry.get('description', ''))
                    link = self._safe_get_str(entry.get('link'))
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                    if not title or len(title) < 10:
                        continue
                    
                    published = self._parse_date(entry)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if link in self.seen_links or title in self.seen_titles:
                        continue
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                    text = f"{title} {summary}".lower()
                    category = self._determine_category(text)
                    
                    # –ò—â–µ–º —Ç–∏–∫–µ—Ä—ã
                    related_tickers = self._find_tickers(text)
                    
                    # –í–∞–∂–Ω–æ—Å—Ç—å
                    importance = self._calculate_importance(title, source_name)
                    
                    # –Ø–∑—ã–∫ (–ø—Ä–∏–º–∏—Ç–∏–≤–Ω–æ)
                    language = 'ru' if any(c in title.lower() for c in ['–∞','–±','–≤','–≥','–¥']) else 'en'
                    
                    # –û–±—Ä–µ–∑–∞–µ–º summary –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                    if len(summary) > 300:
                        summary = summary[:300] + '...'
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    image_path = None
                    
                    if link:
                        image_path = self._extract_image_from_url(str(link), source_name)

                    news_item = NewsItem(
                        source=source_name,
                        title=title,
                        summary=summary,
                        link=link,
                        published=published,
                        related_tickers=related_tickers,
                        importance=importance,
                        language=language,
                        category=category,
                        image_path=image_path,
                    )
                    
                    news_list.append(news_item)
                    
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏: {e}")
                    continue
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {source_name}: {e}")
        
        return news_list
    
    def _determine_category(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–æ–≤–æ—Å—Ç–∏"""
        categories = {
            'macro': ['–≤–≤–ø', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '—Å—Ç–∞–≤–∫–∞', '—Ü–±', '–º–∏–Ω—Ñ–∏–Ω', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥–∏'],
            'company': ['–æ—Ç—á–µ—Ç', '–ø—Ä–∏–±—ã–ª—å', '–¥–∏–≤–∏–¥–µ–Ω–¥—ã', '–∞–∫—Ü–∏–∏', '—Å–æ–±—Ä–∞–Ω–∏–µ'],
            'oil_gas': ['–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '–±–∞—Ä—Ä–µ–ª—å', '–≥–∞–∑–ø—Ä–æ–º', '–ª—É–∫–æ–π–ª'],
            'metal': ['–º–µ—Ç–∞–ª–ª', '–∑–æ–ª–æ—Ç–æ', '—Å–µ—Ä–µ–±—Ä–æ', '–º–µ–¥—å', '–Ω–æ—Ä–Ω–∏–∫–µ–ª—å'],
            'bank': ['–±–∞–Ω–∫', '—Å–±–µ—Ä', '–≤—Ç–±', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞'],
            'tech': ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '—è–Ω–¥–µ–∫—Å', 'it', '–∫–æ–º–ø—å—é—Ç–µ—Ä'],
        }
        
        for cat, keywords in categories.items():
            for kw in keywords:
                if kw in text:
                    return cat
        
        return 'finance'
    
    def _find_tickers(self, text: str) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–∏–∫–µ—Ä—ã –≤ —Ç–µ–∫—Å—Ç–µ"""
        found = set()
        text_lower = text.lower()
        
        for ticker, keywords in self.company_tickers.items():
            for keyword in keywords:
                if keyword in text_lower:
                    found.add(ticker)
                    break
        
        return list(found)
    
    def _calculate_importance(self, title: str, source: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏"""
        importance = 1.0
        
        important_words = ['–∫—Ä–∏–∑–∏—Å', '–æ–±–≤–∞–ª', '—Ä–æ—Å—Ç', '–ø–∞–¥–µ–Ω–∏–µ', '—Å–∞–Ω–∫—Ü–∏–∏', 
                          '—Ä–µ–∫–æ—Ä–¥', '–ø—Ä–∏–±—ã–ª—å', '–¥–∏–≤–∏–¥–µ–Ω–¥—ã', '—Å–ª–∏—è–Ω–∏–µ', '–ø–æ–≥–ª–æ—â–µ–Ω–∏–µ',
                          '–≤–æ–π–Ω–∞', '—ç–º–±–∞—Ä–≥–æ', '–¥–µ—Ñ–æ–ª—Ç', '—à–æ–∫']
        
        title_lower = title.lower()
        for word in important_words:
            if word in title_lower:
                importance += 0.3
        
        important_sources = ['interfax', 'tass', 'rbc', 'reuters', 'bloomberg']
        if source in important_sources:
            importance += 0.2
        
        return min(importance, 2.5)

    def _parse_date(self, entry) -> datetime:
        """
        –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ RSS –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è (MSK, UTC+3)
        """
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Ç–µ–∫—É—â–µ–µ –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è
        msk_time = datetime.now() + timedelta(hours=3)
        
        try:
            # –°–ø–æ—Å–æ–± 1: —á–µ—Ä–µ–∑ published_parsed (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∞—Ç–∞ –æ—Ç feedparser)
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                time_tuple = entry.published_parsed
                if time_tuple and len(time_tuple) >= 6:
                    # feedparser –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –≤ UTC
                    dt_utc = datetime(
                        int(time_tuple[0]),  # –≥–æ–¥
                        int(time_tuple[1]),  # –º–µ—Å—è—Ü
                        int(time_tuple[2]),  # –¥–µ–Ω—å
                        int(time_tuple[3]),  # —á–∞—Å
                        int(time_tuple[4]),  # –º–∏–Ω—É—Ç–∞
                        int(time_tuple[5]),  # —Å–µ–∫—É–Ω–¥–∞
                        tzinfo=timezone.utc  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —ç—Ç–æ UTC
                    )
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è (UTC+3)
                    dt_msk = dt_utc.astimezone(timezone(timedelta(hours=3)))
                    # –£–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞—Å–æ–≤–æ–º –ø–æ—è—Å–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    return dt_msk.replace(tzinfo=None)
            
            # –°–ø–æ—Å–æ–± 2: —á–µ—Ä–µ–∑ published (—Å—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π)
            if hasattr(entry, 'published') and entry.published:
                try:
                    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É —Å –¥–∞—Ç–æ–π
                    dt_parsed = dateutil.parser.parse(entry.published)
                    
                    # –ï—Å–ª–∏ –≤ –¥–∞—Ç–µ –Ω–µ—Ç —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å–∞, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —ç—Ç–æ UTC
                    if dt_parsed.tzinfo is None:
                        dt_parsed = dt_parsed.replace(tzinfo=timezone.utc)
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ MSK
                    dt_msk = dt_parsed.astimezone(timezone(timedelta(hours=3)))
                    return dt_msk.replace(tzinfo=None)
                    
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–æ–∫—É –¥–∞—Ç—ã: {e}")
            
            # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ updated (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if hasattr(entry, 'updated') and entry.updated:
                try:
                    dt_parsed = dateutil.parser.parse(entry.updated)
                    if dt_parsed.tzinfo is None:
                        dt_parsed = dt_parsed.replace(tzinfo=timezone.utc)
                    dt_msk = dt_parsed.astimezone(timezone(timedelta(hours=3)))
                    return dt_msk.replace(tzinfo=None)
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å updated: {e}")
                    
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞—Ç—ã: {e}")
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è
        return msk_time
    
    def _deduplicate_news(self, news_list: List[NewsItem]) -> List[NewsItem]:
        """–£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π"""
        seen = set()
        unique = []
        
        for news in news_list:
            key = hashlib.md5(f"{news.title}{news.link}".encode()).hexdigest()
            if key not in seen:
                seen.add(key)
                unique.append(news)
        
        return unique
    
    def get_finance_news(self, hours: int = 24) -> List[NewsItem]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
        all_news = self.fetch_all_news(limit_per_source=3, max_total=100)
        
        cutoff = datetime.now() - timedelta(hours=hours)
        
        return [news for news in all_news if news.published > cutoff]
    
    def get_news_by_ticker(self, ticker: str, hours: int = 24) -> List[NewsItem]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ç–∏–∫–µ—Ä—É –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
        all_news = self.fetch_all_news(limit_per_source=3, max_total=100)
        
        ticker = ticker.upper()
        cutoff = datetime.now() - timedelta(hours=hours)
        
        filtered = [
            news for news in all_news
            if news.published > cutoff and ticker in news.related_tickers
        ]
        
        return filtered


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_news_parser():
    parser = NewsParser(finance_only=True)
    news = parser.fetch_all_news(limit_per_source=2, max_total=20)
    
    print(f"\n{'='*60}")
    print(f"üí∞ –§–ò–ù–ê–ù–°–û–í–´–ï –ù–û–í–û–°–¢–ò ({len(news)})")
    print('='*60)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categories = {}
    for item in news:
        cat = item.category
        categories[cat] = categories.get(cat, 0) + 1
    
    print(f"\nüìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
    for cat, count in categories.items():
        print(f"  {cat}: {count}")
    
    print(f"\n{'='*60}")
    
    for i, item in enumerate(news, 1):
        tickers = f" [{', '.join(item.related_tickers)}]" if item.related_tickers else ""
        print(f"\n{i}. [{item.source}] {item.category} {tickers}")
        print(f"   {item.title[:100]}...")
        print(f"   üïí {item.published.strftime('%H:%M %d.%m')}")


if __name__ == "__main__":
    test_news_parser()